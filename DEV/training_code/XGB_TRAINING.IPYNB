{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs pour l'augmentation progressive de la taille de l'ensemble d'entraînement\n",
    "train_ratio_start = 0.2  # Commence avec 20% des données\n",
    "train_ratio_end = 1.0    # Termine avec 100% des données\n",
    "\n",
    "train_ratio = train_ratio_start + (train_ratio_end - train_ratio_start) * (epoch / max(1, n_epochs-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, log_loss\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from training_code.utils import load_and_preprocess_data, plot_learning_curve,plot_training_metrics\n",
    "\n",
    "\n",
    "# Supprimer les avertissements non nécessaires\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Création des dossiers pour les résultats\n",
    "os.makedirs('figures/xgb', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f299cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_xgb_hyperparameters(X_train, y_train, X_val, y_val, cv=3):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparamètres du modèle XGBoost\n",
    "    Args:\n",
    "        X_train: Caractéristiques d'entraînement\n",
    "        y_train: Étiquettes d'entraînement\n",
    "        X_val: Caractéristiques de validation\n",
    "        y_val: Étiquettes de validation\n",
    "        cv: Nombre de plis pour la validation croisée\n",
    "    Returns:\n",
    "        Meilleurs hyperparamètres et score\n",
    "    \"\"\"\n",
    "    print(f\"Optimisation des hyperparamètres XGBoost...\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 0.2],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        xgb, param_distributions=param_dist, n_iter=15, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    search_time = time.time() - start_time\n",
    "    print(f\"Recherche par grille terminée en {search_time:.2f} secondes\")\n",
    "\n",
    "    val_score = accuracy_score(y_val, grid_search.predict(X_val))\n",
    "    print(f\"Meilleurs hyperparamètres: {grid_search.best_params_}\")\n",
    "    print(f\"Score de validation croisée: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Score sur l'ensemble de validation: {val_score:.4f}\")\n",
    "\n",
    "    return grid_search.best_params_, val_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_xgb_progressive(X_train, y_train, X_val, y_val, X_test, y_test, best_params, n_epochs=25):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle XGBoost de manière progressive en augmentant la taille de l'ensemble d'entraînement\n",
    "    Args:\n",
    "        X_train: Caractéristiques d'entraînement\n",
    "        y_train: Étiquettes d'entraînement\n",
    "        X_val: Caractéristiques de validation\n",
    "        y_val: Étiquettes de validation\n",
    "        X_test: Caractéristiques de test\n",
    "        y_test: Étiquettes de test\n",
    "        best_params: Meilleurs hyperparamètres trouvés\n",
    "        n_epochs: Nombre d'époques d'entraînement\n",
    "    Returns:\n",
    "        Historique des métriques et meilleur modèle\n",
    "    \"\"\"\n",
    "    print(f\"Entraînement progressif du XGBoost sur {n_epochs} époques...\")\n",
    "\n",
    "    # Métriques\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_precisions = []\n",
    "    val_precisions = []\n",
    "    train_recalls = []\n",
    "    val_recalls = []\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epoch_train_sizes = []\n",
    "     # Convertir en tableaux NumPy pour éviter les problèmes d'indexation\n",
    "    if not isinstance(X_train, np.ndarray):\n",
    "        X_train = np.array(X_train)\n",
    "    if not isinstance(y_train, np.ndarray):\n",
    "        y_train = np.array(y_train)\n",
    "    if not isinstance(X_val, np.ndarray):\n",
    "        X_val = np.array(X_val)\n",
    "    if not isinstance(y_val, np.ndarray):\n",
    "        y_val = np.array(y_val)\n",
    "    if not isinstance(X_test, np.ndarray):\n",
    "        X_test = np.array(X_test)\n",
    "    if not isinstance(y_test, np.ndarray):\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "    # Initialiser le modèle avec les meilleurs paramètres\n",
    "    model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "    # Meilleur modèle\n",
    "    best_model = None\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Valeurs pour l'augmentation progressive de la taille de l'ensemble d'entraînement\n",
    "    train_ratio_start = 0.2  # Commence avec 20% des données\n",
    "    train_ratio_end = 1.0    # Termine avec 100% des données\n",
    "\n",
    "    # Assurer une répartition équilibrée des classes lors de l'échantillonnage\n",
    "    class_indices = {}\n",
    "    unique_classes = np.unique(y_train)\n",
    "    for cls in unique_classes:\n",
    "        class_indices[cls] = np.where(y_train == cls)[0]\n",
    "\n",
    "    with tqdm(total=n_epochs, desc=\"Entraînement\") as pbar:\n",
    "        for epoch in range(n_epochs):\n",
    "            try:\n",
    "                # Augmentation progressive de la taille de l'entraînement\n",
    "                train_ratio = train_ratio_start + (train_ratio_end - train_ratio_start) * (epoch / max(1, n_epochs-1))\n",
    "                indices = []\n",
    "                for cls in unique_classes:\n",
    "                    n_samples = int(len(class_indices[cls]) * train_ratio)\n",
    "                    cls_sample = np.random.choice(class_indices[cls], n_samples, replace=False)\n",
    "                    indices.extend(cls_sample)\n",
    "                np.random.shuffle(indices)\n",
    "                train_size = len(indices)\n",
    "                epoch_train_sizes.append(train_size)\n",
    "                # Extraire les données d'entraînement pour cette époque\n",
    "                X_epoch = X_train[indices]\n",
    "                y_epoch = y_train[indices]\n",
    "\n",
    "                # Créer et entraîner le modèle avec les meilleurs hyperparamètres\n",
    "                model.fit(X_epoch, y_epoch)\n",
    "\n",
    "                # Évaluations\n",
    "                train_pred = model.predict(X_epoch)\n",
    "                val_pred = model.predict(X_val)\n",
    "\n",
    "                train_accuracies.append(accuracy_score(y_epoch, train_pred))\n",
    "                val_accuracies.append(accuracy_score(y_val, val_pred))\n",
    "                train_precisions.append(precision_score(y_epoch, train_pred, zero_division=0))\n",
    "                val_precisions.append(precision_score(y_val, val_pred, zero_division=0))\n",
    "                train_recalls.append(recall_score(y_epoch, train_pred, zero_division=0))\n",
    "                val_recalls.append(recall_score(y_val, val_pred, zero_division=0))\n",
    "                train_f1s.append(f1_score(y_epoch, train_pred, zero_division=0))\n",
    "                val_f1s.append(f1_score(y_val, val_pred, zero_division=0))\n",
    "\n",
    "                # Calcul des pertes (log loss) si predict_proba est disponible\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    try:\n",
    "                        train_probs = model.predict_proba(X_epoch)\n",
    "                        val_probs = model.predict_proba(X_val)\n",
    "                        # Vérifier la validité des probabilités\n",
    "                        if not np.any(np.isnan(train_probs)) and not np.any(np.isnan(val_probs)):\n",
    "                            train_loss = log_loss(y_epoch, train_probs)\n",
    "                            val_loss = log_loss(y_val, val_probs)\n",
    "                        else:\n",
    "                            train_loss = -np.log(max(0.001, train_accuracies[-1]))\n",
    "                            val_loss = -np.log(max(0.001, val_accuracies[-1]))\n",
    "                    except Exception:\n",
    "                        # En cas d'erreur, utiliser une approximation\n",
    "                        train_loss = -np.log(max(0.001, train_accuracies[-1]))\n",
    "                        val_loss = -np.log(max(0.001, val_accuracies[-1]))\n",
    "                else:\n",
    "                    # Si predict_proba n'est pas disponible, simuler une relation inverse avec l'accuracy\n",
    "                    train_loss = -np.log(max(0.001, train_accuracies[-1]))\n",
    "                    val_loss = -np.log(max(0.001, val_accuracies[-1]))\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'Train Acc': f'{train_accuracies[-1]:.4f}',\n",
    "                    'Val Acc': f'{val_accuracies[-1]:.4f}',\n",
    "                    'Train Size': train_size\n",
    "                })\n",
    "\n",
    "                # Suivre le meilleur modèle\n",
    "                if val_accuracies[-1] > best_val_acc:\n",
    "                    best_val_acc = val_accuracies[-1]\n",
    "                    best_model = model\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur à l'époque {epoch+1}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Si aucun modèle valide n'a été trouvé, utiliser un modèle par défaut\n",
    "    if best_model is None:\n",
    "        print(\"Aucun modèle valide trouvé pendant l'entraînement. Création d'un modèle par défaut.\")\n",
    "        best_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "    try:\n",
    "        plot_training_metrics(\n",
    "            train_accuracies, val_accuracies, train_losses, val_losses, \n",
    "            train_f1s, val_f1s, train_recalls, val_recalls, n_epochs,\n",
    "            algorithm_name=\"XGBoost\", output_dir=\"figures/xgb\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création des graphiques: {str(e)}\")\n",
    "\n",
    "    # Évaluation finale du meilleur modèle sur l'ensemble de test\n",
    "    test_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "    # Sauvegarder\n",
    "    joblib.dump(best_model, \"models/xgb_best.pkl\")\n",
    "    print(\"✅ Meilleur modèle XGBoost sauvegardé\")\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_size(epoch_train_sizes, n_epochs):\n",
    "    \"\"\"\n",
    "    Trace la progression de la taille de l'ensemble d'entraînement\n",
    "    Args:\n",
    "        epoch_train_sizes: Liste des tailles d'entraînement à chaque époque\n",
    "        n_epochs: Nombre d'époques\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_epochs+1), epoch_train_sizes, '-o', linewidth=2, markersize=4, color='#2ca02c')\n",
    "    plt.title('Progression de la taille de l\\'ensemble d\\'entraînement', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Époque', fontsize=12)\n",
    "    plt.ylabel('Nombre d\\'échantillons', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/xgb/xgb_training_size.png', dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18be19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(conf_matrix):\n",
    "    \"\"\"\n",
    "    Trace la matrice de confusion\n",
    "    Args:\n",
    "        conf_matrix: Matrice de confusion\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Attack'], yticklabels=['Normal', 'Attack'])\n",
    "    plt.title('Matrice de confusion (Ensemble de test)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Classe prédite', fontsize=12)\n",
    "    plt.ylabel('Classe réelle', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/xgb/xgb_confusion_matrix.png', dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb216463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(data_path=\"UNSW_NB15_training-set.csv\", test_size=0.2, val_size=0.15, n_epochs=25, random_state=42):\n",
    "    \"\"\"\n",
    "    Fonction principale qui exécute tout le pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Pipeline d'analyse et d'entraînement XGBoost pour la détection d'intrusion réseau\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler, encoders = load_and_preprocess_data(\n",
    "        filepath=data_path, test_size=test_size, val_size=val_size, random_state=random_state\n",
    "    )\n",
    "    joblib.dump(scaler, \"models/scaler_xgb.pkl\")\n",
    "    joblib.dump(encoders, \"models/label_encoders_xgb.pkl\")\n",
    "\n",
    "    # Optimiser les hyperparamètres\n",
    "    best_params, val_score = optimize_xgb_hyperparameters(X_train, y_train, X_val, y_val, cv=3)\n",
    "    # Tracer la courbe d'apprentissage pour évaluer l'impact de la taille de l'ensemble d'entraînement\n",
    "    model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    plot_learning_curve(model, X_train, y_train)\n",
    "    # Entraînement progressif\n",
    "    results = train_xgb_progressive(X_train, y_train, X_val, y_val, X_test, y_test, best_params, n_epochs=n_epochs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTemps total d'exécution: {elapsed_time:.2f} secondes ({elapsed_time / 60:.2f} minutes)\")\n",
    "    print(\"\\n📊 Résumé des performances:\")\n",
    "    print(f\"Accuracy finale sur le test: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Precision finale sur le test: {results['test_precision']:.4f}\")\n",
    "    print(f\"Recall final sur le test: {results['test_recall']:.4f}\")\n",
    "    print(f\"F1-Score final sur le test: {results['test_f1']:.4f}\")\n",
    "    print(f\"Meilleurs hyperparamètres: {best_params}\")\n",
    "\n",
    "    # Évaluation finale du modèle\n",
    "    print(\"🔍 Analyse de la matrice de confusion:\")\n",
    "    conf_matrix = results['confusion_matrix']\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    total = tn + fp + fn + tp\n",
    "    print(f\" - Vrais Négatifs (TN): {tn} ({tn/total*100:.2f}%)\")\n",
    "    print(f\" - Faux Positifs (FP): {fp} ({fp/total*100:.2f}%)\")\n",
    "    print(f\" - Faux Négatifs (FN): {fn} ({fn/total*100:.2f}%)\")\n",
    "    print(f\" - Vrais Positifs (TP): {tp} ({tp/total*100:.2f}%)\")\n",
    "    print(f\" - Taux de faux positifs: {fp/(fp+tn)*100:.2f}%\")\n",
    "    print(f\" - Taux de faux négatifs: {fn/(fn+tp)*100:.2f}%\")\n",
    "\n",
    "\n",
    "    # Tracer la matrice de confusion pour le meilleur modèle\n",
    "    plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
